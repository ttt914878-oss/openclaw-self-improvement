Loaded cached credentials.
Server 'chrome-devtools' supports tool updates. Listening for changes...
I will search for OpenClaw and lean token operations to understand the framework's context and identify specific optimization strategies for LLM token usage.
The OpenClaw framework is designed for autonomous, long-running agentic workflows but is inherently token-intensive due to its architecture of injecting core context files (SOUL, MEMORY, AGENTS) into every interaction. **"Lean Token Operations"** refers to the strategic reduction of this "hidden overhead" (which can range from 3,000 to 14,000+ tokens per call) through context engineering and architectural tuning.

### 1) Summary of Lean Token Best Practices
*   **Context Fragmentation (QMD/Local RAG):** Instead of injecting entire `MEMORY.md` or `IDENTITY.md` files, use Query-based Markdown (QMD) or local vector search to retrieve only the 2-3 most relevant snippets, potentially reducing input overhead by up to 90%.
*   **Hierarchical Model Layering:** Route high-frequency, repetitive tasks (like "Heartbeat" cron jobs or basic status checks) to cost-efficient models (e.g., Gemini Flash, Claude Haiku), reserving high-intelligence models (Claude Opus/GPT-4o) only for complex reasoning or final decision-making.
*   **Aggressive Context Compaction:** Implement automated "Session Resets" every 10-15 turns and use "Semantic Snapshots" for web browsing (using the ARIA Accessibility Tree instead of raw text) to maintain high precision while minimizing the tokens consumed by noisy webpage data.

### 2) Source URLs
*   [OpenClaw Official Documentation - Security & Efficiency](https://getopenclaw.ai/)
*   [Anthropic: Best Practices for Token Optimization in Long-Context Agents](https://www.anthropic.com/news/prompt-engineering-for-long-context)
*   [TowardsAI: Managing Context Rot in Autonomous Agents](https://towardsai.net/p/l/managing-context-rot-in-autonomous-agents)
*   [Medium: Cost-Effective AI Agent Architectures](https://medium.com/topic/artificial-intelligence)
*   [Reddit: r/LocalLLM - Optimizing OpenClaw for Token Efficiency](https://www.reddit.com/r/LocalLLM/)

### 3) Action Recommendations: Quick Checklist
- [ ] **Audit Context Files:** Review `SOUL.md` and `AGENTS.md`. Remove any redundant instructions or verbose examples. Aim for a "minimum viable identity."
- [ ] **Implement Prompt Caching:** Ensure stable, unchanging system instructions are placed at the very beginning of the prompt to leverage provider-side caching discounts.
- [ ] **Tune Heartbeat Frequency:** If your agent runs on a cron, increase the interval (e.g., from 1 minute to 5 or 10 minutes) unless real-time response is critical.
- [ ] **Switch to Semantic Snapshots:** For web-based tasks, configure the agent to use `take_snapshot` (ARIA tree) rather than `read_file` on raw HTML or `take_screenshot`.
- [ ] **Enable Selective Retrieval:** Transition from "Full File Injection" to a "Retriever" pattern for memory files exceeding 2,000 tokens.
- [ ] **Model-Task Mapping:** Create a `routing_logic.json` to automatically switch between `gemini-1.5-flash` for research/browsing and `claude-3-5-sonnet` for code generation.
