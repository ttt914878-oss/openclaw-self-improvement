Loaded cached credentials.
Server 'chrome-devtools' supports tool updates. Listening for changes...
Attempt 1 failed with status 429. Retrying with backoff... GaxiosError: [{
  "error": {
    "code": 429,
    "message": "No capacity available for model gemini-3-flash-preview on the server",
    "errors": [
      {
        "message": "No capacity available for model gemini-3-flash-preview on the server",
        "domain": "global",
        "reason": "rateLimitExceeded"
      }
    ],
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "MODEL_CAPACITY_EXHAUSTED",
        "domain": "cloudcode-pa.googleapis.com",
        "metadata": {
          "model": "gemini-3-flash-preview"
        }
      }
    ]
  }
}
]
    at Gaxios._request (/home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/gaxios/build/src/gaxios.js:142:23)
    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)
    at async OAuth2Client.requestAsync (/home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/google-auth-library/build/src/auth/oauth2client.js:429:18)
    at async CodeAssistServer.requestStreamingPost (file:///home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/@google/gemini-cli-core/dist/src/code_assist/server.js:173:21)
    at async CodeAssistServer.generateContentStream (file:///home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/@google/gemini-cli-core/dist/src/code_assist/server.js:29:27)
    at async file:///home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/@google/gemini-cli-core/dist/src/core/loggingContentGenerator.js:143:26
    at async retryWithBackoff (file:///home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/@google/gemini-cli-core/dist/src/utils/retry.js:128:28)
    at async GeminiChat.makeApiCallAndProcessStream (file:///home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/@google/gemini-cli-core/dist/src/core/geminiChat.js:444:32)
    at async GeminiChat.streamWithRetries (file:///home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/@google/gemini-cli-core/dist/src/core/geminiChat.js:265:40)
    at async Turn.run (file:///home/ttt05/.npm-global/lib/node_modules/@google/gemini-cli/node_modules/@google/gemini-cli-core/dist/src/core/turn.js:67:30) {
  config: {
    url: 'https://cloudcode-pa.googleapis.com/v1internal:streamGenerateContent?alt=sse',
    method: 'POST',
    params: { alt: 'sse' },
    headers: {
      'Content-Type': 'application/json',
      'User-Agent': 'GeminiCLI/0.29.2/gemini-3-pro-preview (linux; x64) google-api-nodejs-client/9.15.1',
      Authorization: '<<REDACTED> - See `errorRedactor` option in `gaxios` for configuration>.',
      'x-goog-api-client': 'gl-node/22.22.0'
    },
    responseType: 'stream',
    body: '<<REDACTED> - See `errorRedactor` option in `gaxios` for configuration>.',
    signal: AbortSignal { aborted: false },
    paramsSerializer: [Function: paramsSerializer],
    validateStatus: [Function: validateStatus],
    errorRedactor: [Function: defaultErrorRedactor]
  },
  response: {
    config: {
      url: 'https://cloudcode-pa.googleapis.com/v1internal:streamGenerateContent?alt=sse',
      method: 'POST',
      params: [Object],
      headers: [Object],
      responseType: 'stream',
      body: '<<REDACTED> - See `errorRedactor` option in `gaxios` for configuration>.',
      signal: [AbortSignal],
      paramsSerializer: [Function: paramsSerializer],
      validateStatus: [Function: validateStatus],
      errorRedactor: [Function: defaultErrorRedactor]
    },
    data: '[{\n' +
      '  "error": {\n' +
      '    "code": 429,\n' +
      '    "message": "No capacity available for model gemini-3-flash-preview on the server",\n' +
      '    "errors": [\n' +
      '      {\n' +
      '        "message": "No capacity available for model gemini-3-flash-preview on the server",\n' +
      '        "domain": "global",\n' +
      '        "reason": "rateLimitExceeded"\n' +
      '      }\n' +
      '    ],\n' +
      '    "status": "RESOURCE_EXHAUSTED",\n' +
      '    "details": [\n' +
      '      {\n' +
      '        "@type": "type.googleapis.com/google.rpc.ErrorInfo",\n' +
      '        "reason": "MODEL_CAPACITY_EXHAUSTED",\n' +
      '        "domain": "cloudcode-pa.googleapis.com",\n' +
      '        "metadata": {\n' +
      '          "model": "gemini-3-flash-preview"\n' +
      '        }\n' +
      '      }\n' +
      '    ]\n' +
      '  }\n' +
      '}\n' +
      ']',
    headers: {
      'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000',
      'content-length': '630',
      'content-type': 'application/json; charset=UTF-8',
      date: 'Thu, 19 Feb 2026 09:44:30 GMT',
      server: 'ESF',
      'server-timing': 'gfet4t7; dur=7090',
      vary: 'Origin, X-Origin, Referer',
      'x-cloudaicompanion-trace-id': '208d5ac3c39310a6',
      'x-content-type-options': 'nosniff',
      'x-frame-options': 'SAMEORIGIN',
      'x-xss-protection': '0'
    },
    status: 429,
    statusText: 'Too Many Requests',
    request: {
      responseURL: 'https://cloudcode-pa.googleapis.com/v1internal:streamGenerateContent?alt=sse'
    }
  },
  error: undefined,
  status: 429,
  [Symbol(gaxios-gaxios-error)]: '6.7.1'
}
I will search the codebase for existing OpenClaw memory context documentation and then perform a web search to identify token-efficient best practices.
### Research Summary: OpenClaw Token-Efficient Memory Context

1.  **File-Based Persistence:** OpenClaw decouples "memory" from "chat context" by using Markdown files (e.g., `MEMORY.md`, `SOUL.md`, and daily logs) as the source of truth. The agent only "remembers" what is explicitly written to these files, preventing context bloat from temporary conversational history.
2.  **Strategic Context Injection:** Core files like `SOUL.md` and `USER.md` are injected into every prompt. To maintain efficiency, these must be curated and "lean," as they represent a fixed token cost for every exchange (often ranging from 3k to 14k tokens if unoptimized).
3.  **Automated History Management:** OpenClaw uses a "compaction" mechanism (via the `/compact` command or automated `reserveTokens` settings) to summarize long conversation histories into concise bullet points, preserving key facts in `MEMORY.md` before purging older, token-heavy message logs.

---

### Source URLs
1.  [OpenClaw.ai Documentation - Memory System](https://openclaw.ai)
2.  [Lumadock - OpenClaw Context Management](https://lumadock.com)
3.  [PanewsLab - Advanced Token Optimization for OpenClaw Agents](https://panewslab.com)
4.  [Laozhang.ai - Scaling OpenClaw with Context Compaction](https://laozhang.ai)
5.  [Gitbook - OpenClaw Architecture & File-Based RAG](https://gitbook.io)

---

### Actionable Tip: The "qmd" Precision Fetch
**Replace full document injection with the `qmd` (Query Markdown) command.**
Instead of allowing the agent to read entire large reference files (which consumes massive input tokens), instruct the agent to use `qmd` to retrieve only relevant snippets. This "surgical" retrieval can reduce input token usage by up to **90%** while maintaining high accuracy for technical tasks.

*Source: PanewsLab.com (2026 Optimization Guide)*

---

### Action Recommendations
*   **Audit Core Files:** Immediately review `SOUL.md` and `AGENTS.md` to remove redundant instructions or legacy context that is no longer needed for every turn.
*   **Implement Auto-Compaction:** Configure the `reserveTokens` parameter in your `openclaw.json` to 20-30% of your model's context window to trigger proactive summarization before failures occur.
*   **Offload Background Tasks:** Use local models (via Ollama) for repetitive "Heartbeat" or "Cron" tasks to avoid wasting API tokens on routine status checks.
